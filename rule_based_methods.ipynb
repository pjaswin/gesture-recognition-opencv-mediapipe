{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddabe6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 10:59:10.787315: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-11 10:59:11.145049: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-11 10:59:11.323916: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-11 10:59:11.377336: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-11 10:59:11.709101: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-11 10:59:13.868528: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "982fdd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749632356.384037    8573 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1749632356.392254   10882 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 Mesa 24.2.8-1ubuntu1~24.04.1), renderer: AMD Radeon 780M (radeonsi, gfx1103_r1, LLVM 19.1.1, DRM 3.61, 6.11.0-26-generic)\n"
     ]
    }
   ],
   "source": [
    "mp_hands =mp.solutions.hands\n",
    "mp.drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=2, min_detection_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2837f53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_gesture(landmarks):\n",
    "    def distance(a, b):\n",
    "        return np.linalg.norm(np.array([a.x, a.y]) - np.array([b.x, b.y]))\n",
    "\n",
    "    def norm_distance(a, b):\n",
    "        return distance(a, b) / ref_len\n",
    "\n",
    "    def is_finger_extended(tip, pip):\n",
    "        return tip.y < pip.y\n",
    "\n",
    "    def is_finger_curled(tip, pip, dip):\n",
    "        return tip.y > pip.y and dip.y > pip.y\n",
    "\n",
    "    # Landmark indices\n",
    "    wrist = landmarks[0]\n",
    "    thumb_tip, thumb_ip = landmarks[4], landmarks[3]\n",
    "    index_tip, index_dip, index_pip = landmarks[8], landmarks[7], landmarks[6]\n",
    "    middle_tip, middle_dip, middle_pip = landmarks[12], landmarks[11], landmarks[10]\n",
    "    ring_tip, ring_dip, ring_pip = landmarks[16], landmarks[15], landmarks[14]\n",
    "    pinky_tip, pinky_dip, pinky_pip = landmarks[20], landmarks[19], landmarks[18]\n",
    "    middle_mcp = landmarks[9]\n",
    "\n",
    "    # Normalize distances\n",
    "    ref_len = distance(wrist, middle_mcp) + 1e-6\n",
    "\n",
    "    d_thumb = norm_distance(wrist, thumb_tip)\n",
    "    d_index = norm_distance(wrist, index_tip)\n",
    "    d_middle = norm_distance(wrist, middle_tip)\n",
    "    d_ring = norm_distance(wrist, ring_tip)\n",
    "    d_pinky = norm_distance(wrist, pinky_tip)\n",
    "\n",
    "    thumb_index_dist = norm_distance(thumb_tip, index_tip)\n",
    "\n",
    "    # Finger states\n",
    "    index_extended = is_finger_extended(index_tip, index_pip)\n",
    "    middle_extended = is_finger_extended(middle_tip, middle_pip)\n",
    "    ring_extended = is_finger_extended(ring_tip, ring_pip)\n",
    "    pinky_extended = is_finger_extended(pinky_tip, pinky_pip)\n",
    "\n",
    "    index_curled = is_finger_curled(index_tip, index_pip, index_dip)\n",
    "    middle_curled = is_finger_curled(middle_tip, middle_pip, middle_dip)\n",
    "    ring_curled = is_finger_curled(ring_tip, ring_pip, ring_dip)\n",
    "    pinky_curled = is_finger_curled(pinky_tip, pinky_pip, pinky_dip)\n",
    "\n",
    "    # === Gesture Logic ===\n",
    "\n",
    "    # ‚úä Fist: All fingers curled\n",
    "    if all([index_curled, middle_curled, ring_curled, pinky_curled]) and d_index < 0.25:\n",
    "        return \"fist\"\n",
    "\n",
    "    # ‚úã Palm: All fingers extended and spread\n",
    "    elif all([index_extended, middle_extended, ring_extended, pinky_extended]) and min(d_index, d_middle, d_ring, d_pinky) > 0.35:\n",
    "        return \"palm\"\n",
    "\n",
    "    # üëç Like: Thumb extended, others curled\n",
    "    elif d_thumb > 0.4 and all([index_curled, middle_curled, ring_curled, pinky_curled]):\n",
    "        return \"like\"\n",
    "\n",
    "    # ‚úåÔ∏è Peace: Index & middle extended, ring & pinky curled\n",
    "    elif index_extended and middle_extended and ring_curled and pinky_curled:\n",
    "        return \"peace\"\n",
    "\n",
    "    # üëå OK: Thumb & index tips close, middle extended\n",
    "    elif thumb_index_dist < 0.2 and middle_extended:\n",
    "        return \"ok\"\n",
    "\n",
    "    # ü§ô Call Me: Thumb & pinky extended, others curled\n",
    "    elif d_thumb > 0.3 and d_pinky > 0.3 and all([index_curled, middle_curled, ring_curled]):\n",
    "        return \"call\"\n",
    "\n",
    "    return \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "948ce523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise ValueError(\"Image not found or unable to read.\")\n",
    "\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(image_rgb)\n",
    "\n",
    "    gesture = \"no hands\"\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Crop and zoom\n",
    "            zoomed_image = crop_and_resize_hand(image, hand_landmarks.landmark)\n",
    "            zoomed_rgb = cv2.cvtColor(zoomed_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Re-run MediaPipe on zoomed hand\n",
    "            refined_results = hands.process(zoomed_rgb)\n",
    "\n",
    "            if refined_results.multi_hand_landmarks:\n",
    "                refined_landmarks = refined_results.multi_hand_landmarks[0]\n",
    "                gesture = get_gesture(refined_landmarks.landmark)\n",
    "            else:\n",
    "                gesture = get_gesture(hand_landmarks.landmark)\n",
    "\n",
    "    return gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cdb972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize_hand(image, landmarks, size=(256, 256), margin=40):\n",
    "    h, w, _ = image.shape\n",
    "    xs = [lm.x * w for lm in landmarks]\n",
    "    ys = [lm.y * h for lm in landmarks]\n",
    "\n",
    "    min_x = int(max(min(xs) - margin, 0))\n",
    "    max_x = int(min(max(xs) + margin, w))\n",
    "    min_y = int(max(min(ys) - margin, 0))\n",
    "    max_y = int(min(max(ys) + margin, h))\n",
    "\n",
    "    cropped = image[min_y:max_y, min_x:max_x]\n",
    "    if cropped.size == 0:\n",
    "        return image  # fallback if crop failed\n",
    "    return cv2.resize(cropped, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae2d6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749632356.438124   10858 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1749632356.468076   10864 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "Processing call:   0%|          | 0/28193 [00:00<?, ?img/s]W0000 00:00:1749632356.527751   10871 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "Processing call: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28193/28193 [32:45<00:00, 14.34img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for call: 28193\n",
      "Correct gestures for call: 3987\n",
      "Accuracy for call: 14.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing fist: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27764/27764 [30:10<00:00, 15.33img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for fist: 27764\n",
      "Correct gestures for fist: 2\n",
      "Accuracy for fist: 0.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing like: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27721/27721 [32:02<00:00, 14.42img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for like: 27721\n",
      "Correct gestures for like: 6293\n",
      "Accuracy for like: 22.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ok: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27999/27999 [30:08<00:00, 15.48img/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for ok: 27999\n",
      "Correct gestures for ok: 14969\n",
      "Accuracy for ok: 53.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing palm: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28326/28326 [37:23<00:00, 12.63img/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for palm: 28326\n",
      "Correct gestures for palm: 23666\n",
      "Accuracy for palm: 83.55%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing peace: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28303/28303 [29:13<00:00, 16.14img/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images for peace: 28303\n",
      "Correct gestures for peace: 22107\n",
      "Accuracy for peace: 78.11%\n",
      "\n",
      "Total images processed: 168306\n",
      "Correct gestures: 71024\n",
      "Accuracy: 42.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "__path__ = \"dataset/hagrid-sample-500k-384p/hagrid_500k\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def main():\n",
    "    import os\n",
    "    counter = 0\n",
    "    correct = 0\n",
    "    filename = [\"train_val_call\", \"train_val_fist\", \"train_val_like\", \"train_val_ok\", \"train_val_palm\", \"train_val_peace\"]\n",
    "\n",
    "    for file in filename:\n",
    "        label = file[10:]\n",
    "        counter_label = 0\n",
    "        correct_label = 0\n",
    "        folder_path = os.path.join(__path__, file)\n",
    "        image_files = [f for f in os.listdir(folder_path) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "        for f in tqdm(image_files, desc=f\"Processing {label}\", unit=\"img\"):\n",
    "            image_path = os.path.join(folder_path, f)\n",
    "            gesture = process_image(image_path)\n",
    "\n",
    "            if gesture == label:\n",
    "                correct += 1\n",
    "                correct_label += 1\n",
    "            counter += 1\n",
    "            counter_label += 1\n",
    "\n",
    "        print(f\"\\nTotal images for {label}: {counter_label}\")\n",
    "        print(f\"Correct gestures for {label}: {correct_label}\")\n",
    "        print(f\"Accuracy for {label}: {correct_label / counter_label * 100:.2f}%\\n\")\n",
    "\n",
    "    print(f\"Total images processed: {counter}\")\n",
    "    print(f\"Correct gestures: {correct}\")\n",
    "    print(f\"Accuracy: {correct / counter * 100:.2f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430e2491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97fc8b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
